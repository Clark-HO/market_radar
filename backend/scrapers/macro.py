import requests
import pandas as pd
import yfinance as yf
import json
import time
import os
from datetime import datetime
from io import StringIO

# ç¢ºä¿è·¯å¾‘æ­£ç¢º (å­˜åˆ° E:\antigravity\market_radar\frontend\public\macro_data.json)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
PUBLIC_DIR = os.path.join(BASE_DIR, "frontend", "public")
if not os.path.exists(PUBLIC_DIR):
    os.makedirs(PUBLIC_DIR)
    
MACRO_DATA_FILE = os.path.join(PUBLIC_DIR, "macro_data.json")

class MacroScraper:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

    def fetch_yahoo_safe(self, ticker_id, name):
        """
        [æ–°å¢] å®‰å…¨æŠ“å– Yahoo æ•¸æ“šçš„é€šç”¨å‡½å¼
        åŒ…å«ï¼šè‡ªå‹•é‡è©¦æ©Ÿåˆ¶ (Retry Logic)
        """
        print(f"   -> Fetching {name} ({ticker_id})...")
        retries = [5, 10, 15] # ç¬¬ä¸€æ¬¡å¤±æ•—ç­‰5ç§’ï¼Œç¬¬äºŒæ¬¡10ç§’...
        
        for i, delay in enumerate(retries):
            try:
                # ä½¿ç”¨ fast_info æ¯”è¼ƒå¿«ä¸”ä¸æ˜“è¢«æ“‹
                t = yf.Ticker(ticker_id)
                price = t.fast_info.last_price
                
                # å˜—è©¦æŠ“æ­·å²è¨ˆç®—æ¼²è·Œå¹…
                change = 0
                change_pct = 0
                try:
                    hist = t.history(period="2d")
                    if len(hist) >= 2:
                        curr = hist.iloc[-1]['Close']
                        prev = hist.iloc[-2]['Close']
                        change = curr - prev
                        change_pct = (change / prev) * 100
                except:
                    pass # å¦‚æœæŠ“ä¸åˆ°æ­·å²å°±ç®—äº†ï¼Œè‡³å°‘æœ‰ç¾åƒ¹
                
                return {
                    "price": round(price, 2),
                    "change": round(change, 2),
                    "change_percent": round(change_pct, 2)
                }
                
            except Exception as e:
                if i < len(retries) - 1:
                    print(f"      âš ï¸ Yahoo Busy. Retrying in {delay}s... ({i+1}/{len(retries)})")
                    time.sleep(delay)
                else:
                    print(f"      âŒ Failed to fetch {name}: {e}")
                    return None
        return None

    def fetch_taiex(self):
        """æŠ“å–åŠ æ¬ŠæŒ‡æ•¸ (^TWII)"""
        data = self.fetch_yahoo_safe("^TWII", "TAIEX")
        if data:
            return {
                "taiex_close": data['price'],
                "change": data['change'],
                "change_percent": data['change_percent']
            }
        return None

    def fetch_currency(self):
        """æŠ“å–åŒ¯ç‡ (USDTWD=X)"""
        data = self.fetch_yahoo_safe("USDTWD=X", "USD/TWD")
        if data:
            trend = "Stable"
            if data['change'] > 0: trend = "Depreciating" # å°å¹£è²¶å€¼ (USDè®Šè²´)
            elif data['change'] < 0: trend = "Appreciating"
            
            return {
                "usd_twd": data['price'],
                "trend": trend
            }
        return {"usd_twd": 32.0, "trend": "Stable"}

    def fetch_futures_oi(self):
        """
        æŠ“å–æœŸäº¤æ‰€å¤–è³‡ç©ºå–® (ä¿ç•™ä½ åŸæœ¬å„ªç§€çš„é‚è¼¯)
        """
        print("   -> Fetching Futures OI (TAIFEX Scraper)...")
        url = "https://www.taifex.com.tw/cht/3/futContractsDate"
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            
            dfs = pd.read_html(StringIO(response.text), match="æœŸè²¨")
            
            for i, df in enumerate(dfs):
                # print(f"      ğŸ‘€ Inspecting Table {i} Shape: {df.shape}")
                
                df = df.reset_index()
                
                # è™•ç† MultiIndex æ¬„ä½åç¨±
                if isinstance(df.columns, pd.MultiIndex):
                     df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]
                
                # é—œéµå­—æœå°‹æ¬„ä½
                col_contract = next((c for c in df.columns if "å¥‘ç´„" in c or "å•†å“" in c), None)
                col_identity = next((c for c in df.columns if "èº«" in c), None)
                col_net_oi   = next((c for c in df.columns if "å¤šç©ºæ·¨é¡" in c and "æœªå¹³å€‰" in c), None)
                
                if col_contract and col_identity and col_net_oi:
                    # å¡«è£œç©ºç™½æ¬„ä½ (Forward Fill)
                    df[col_contract] = df[col_contract].ffill()
                    
                    # ç¯©é¸ï¼šè‡ºè‚¡æœŸè²¨ + å¤–è³‡ (æ’é™¤å°å‹)
                    target_row = df[
                        (df[col_contract].astype(str).str.contains("è‡ºè‚¡æœŸè²¨")) &
                        (~df[col_contract].astype(str).str.contains("å°å‹")) & 
                        (df[col_identity].astype(str).str.contains("å¤–è³‡"))
                    ]
                    
                    if not target_row.empty:
                        raw_val = target_row.iloc[0][col_net_oi]
                        try:
                            net_oi = int(str(raw_val).replace(",", "").strip())
                            print(f"      âœ… Found Foreign Futures Net OI: {net_oi}")
                            return net_oi
                        except:
                            print(f"      âš ï¸ Parse Error for value: {raw_val}")

            print("      âš ï¸ Scraper finished but could not find target row.")
            return -35000 
            
        except Exception as e:
            print(f"      âš ï¸ Futures fetch failed: {e}")
            return -5000 

    def fetch_sector_flow(self):
        """
        æŠ“å–çœŸå¯¦é¡è‚¡è³‡é‡‘æµå‘ (TWSE BFIAMU)
        """
        print("   -> Fetching Sector Flow (TWSE BFIAMU)...")
        
        # å˜—è©¦æŠ“å–æœ€æ–°äº¤æ˜“æ—¥ (å›æº¯ 5 å¤©)
        date_check = datetime.now()
        found_data = None
        
        for _ in range(5):
            date_str = date_check.strftime("%Y%m%d")
            url = f"https://www.twse.com.tw/rwd/zh/afterTrading/BFIAMU?date={date_str}&response=json"
            try:
                r = requests.get(url, headers=self.headers, timeout=10)
                data = r.json()
                if data.get('stat') == 'OK':
                    found_data = data
                    print(f"      âœ… Found Sector Data for {date_str}")
                    break
            except:
                pass
            date_check -= timedelta(days=1)
            
        if not found_data:
            print("      âš ï¸ Sector Flow fetch failed. Using empty list.")
            return []

        # Parse Data
        # Fields: ['åˆ†é¡æŒ‡æ•¸åç¨±', 'æˆäº¤è‚¡æ•¸', 'æˆäº¤é‡‘é¡', 'æˆäº¤ç­†æ•¸', 'æ¼²è·ŒæŒ‡æ•¸']
        # Index 0: Name, Index 2: Value
        try:
            raw_sectors = []
            total_value = 0
            
            for row in found_data.get('data', []):
                name = row[0]
                val_str = row[2]
                try:
                    val = float(val_str.replace(',', ''))
                    total_value += val
                    # Filter out "Total" (ç¸½è¨ˆ) or specific aggregate rows if any
                    if name not in ['ç™¼è¡Œé‡åŠ æ¬Šè‚¡åƒ¹æŒ‡æ•¸', 'æœªå«é‡‘èä¿éšªè‚¡æŒ‡æ•¸', 'æœªå«é›»å­è‚¡æŒ‡æ•¸', 'æœªå«é‡‘èé›»å­è‚¡æŒ‡æ•¸']:
                         raw_sectors.append({"name": name, "value": val})
                except:
                    continue
            
            # Sort by Value Desc
            raw_sectors.sort(key=lambda x: x['value'], reverse=True)
            
            # Take Top 5 and Aggregate Others
            top_5 = raw_sectors[:5]
            others_val = sum(x['value'] for x in raw_sectors[5:])
            
            result = []
            
            # Helper to determine trend (Hot/Cool) - simplified logic based on Ratio?
            # Or usually we compare to yesterday. 
            # For now, just labels based on volume dominance.
            
            for s in top_5:
                ratio = (s['value'] / total_value) * 100
                trend = "Normal"
                if ratio > 30: trend = "Hot"
                elif ratio < 5: trend = "Cool"
                
                # Normalize names (Remove "é¡")
                display_name = s['name'].replace("é¡", "")
                
                result.append({
                    "name": display_name,
                    "ratio": round(ratio, 1),
                    "trend": trend
                })
            
            # Add Others
            if others_val > 0:
                others_ratio = (others_val / total_value) * 100
                result.append({
                    "name": "å…¶ä»–",
                    "ratio": round(others_ratio, 1),
                    "trend": "Normal"
                })
                
            return result
            
        except Exception as e:
            print(f"      âš ï¸ Sector parsing failed: {e}")
            return []

    def run(self):
        print("ğŸš€ [Macro Worker] Starting Update (Robust Mode)...")
        
        # 1. å¼·åˆ¶ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…è·Ÿä¸Šä¸€æ”¯ç¨‹å¼ (data_updater) æ¶é »å¯¬è¢« Yahoo å°é–
        time.sleep(2)
        
        # 2. TAIEX
        taiex = self.fetch_taiex()
        if not taiex:
             taiex = {"taiex_close": 0.0, "change": 0.0, "change_percent": 0.0}

        # 3. Futures
        net_oi = self.fetch_futures_oi()
        
        # åˆ¤æ–·å¤šç©ºè¨Šè™Ÿ
        fut_status = "Neutral"
        color = "gray"
        if net_oi < -15000: # ç©ºå–®è¶…é 1.5è¬å£
            fut_status = "Bearish Alert"
            color = "red" # ä¾ç…§ä½ çš„è¨­å®š: ç´…è‰²ä»£è¡¨è­¦æˆ’
        elif net_oi > 10000:
             fut_status = "Bullish"
             color = "green"
        
        futures_data = {
            "futures_net_oi": net_oi,
            "futures_status": fut_status,
            "futures_color": color
        }

        # 4. Currency
        curr = self.fetch_currency()

        # 5. Sector
        volume = self.fetch_sector_flow()
        
        # Assemble
        data = {
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "market_status": taiex,
            "chips": futures_data,
            "currency": curr,
            "sector_flow": volume
        }
        
        with open(MACRO_DATA_FILE, "w", encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
            
        print(f"âœ… [Macro Worker] Data saved to {MACRO_DATA_FILE}")

if __name__ == "__main__":
    scraper = MacroScraper()
    scraper.run()